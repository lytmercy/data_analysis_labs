{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laboratory work #3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all significant libraries for this project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow & Keras Libraries\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Input, Rescaling\n",
    "from keras.layers import Conv2D, MaxPool2D, RNN\n",
    "from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Import scikit-learn libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import libraries for text cleaning\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Who\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Possible Asteroid Impacts with Earth (from [kaggle](https://www.kaggle.com/datasets/nasa/asteroid-impacts))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Loading dataset to pandas DataFrame\n",
    "asteroid_df = pd.read_csv(\"datasets\\\\asteroid_dataset\\\\asteroid_classification.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n",
      "       'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for analysis from the dataframe\n",
    "asteroid_df.drop([\"Object Name\", \"Epoch (TDB)\", \"Perihelion Argument (deg)\", \"Node Longitude (deg)\",\n",
    "                  \"Mean Anomoly (deg)\", \"Perihelion Distance (AU)\", \"Aphelion Distance (AU)\",\n",
    "                  \"Minimum Orbit Intersection Distance (AU)\", \"Orbital Reference\"], axis=1, inplace=True)\n",
    "print(asteroid_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15634 entries, 0 to 15634\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Object Classification    15634 non-null  object \n",
      " 1   Orbit Axis (AU)          15634 non-null  float64\n",
      " 2   Orbit Eccentricity       15634 non-null  float64\n",
      " 3   Orbit Inclination (deg)  15634 non-null  float64\n",
      " 4   Orbital Period (yr)      15634 non-null  float64\n",
      " 5   Asteroid Magnitude       15634 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 855.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop null values of dataframe as we have only one null value\n",
    "asteroid_df.dropna(inplace=True)\n",
    "asteroid_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Change class names\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apollo Asteroid\" ,\n",
    "                                          \"Apollo\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Aten Asteroid\",\n",
    "                                          \"Aten\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Amor Asteroid\",\n",
    "                                          \"Amor\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apohele Asteroid\",\n",
    "                                          \"Apohele\", inplace=True)\n",
    "# Drop unnecessary class\n",
    "necessary_class = [\"Apollo\", \"Aten\", \"Amor\"]\n",
    "asteroid_df = asteroid_df[asteroid_df[\"Object Classification\"].isin(necessary_class)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, features):\n",
    "    \"\"\"Function to remove the outliers;\n",
    "    :param dataframe: pandas DataFrame with data;\n",
    "    :param features: list with features from dataframe.\n",
    "    \"\"\"\n",
    "    # Copy dataframe to another variable\n",
    "    dataframe_copy = dataframe.copy()\n",
    "\n",
    "    # Iterate through features\n",
    "    for feature in features:\n",
    "        if dataframe[feature].dtype == object:\n",
    "            continue\n",
    "        # Calculate q1, q3 and iqr\n",
    "        q3 = dataframe[feature].quantile(0.75)\n",
    "        q1 = dataframe[feature].quantile(0.25)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Get local minimum and maximum\n",
    "        local_min = q1 - (1.5 * iqr)\n",
    "        local_max = q3 + (1.5 * iqr)\n",
    "\n",
    "        # Remove the outliers\n",
    "        dataframe_copy = dataframe_copy[(dataframe_copy[feature] >= local_min) &\n",
    "                                        (dataframe_copy[feature] <= local_max)]\n",
    "\n",
    "    return dataframe_copy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the df\n",
    "asteroid_features = asteroid_df.columns.tolist()\n",
    "print(asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Remove outliers from the dataframe\n",
    "asteroid_df = remove_outliers(asteroid_df, asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apollo    6651\n",
      "Amor      5686\n",
      "Aten       965\n",
      "Name: Object Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View count of class names\n",
    "print(asteroid_df.iloc[:, 0].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "num_classes = len(necessary_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Object Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                  Amor         0.807646            0.590282   \n",
      "2                  Amor         0.758732            0.610967   \n",
      "4                  Amor         0.587438            0.469295   \n",
      "8                  Amor         0.570204            0.427279   \n",
      "9                Apollo         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Normalise dataset\n",
    "norm_asteroid_df = asteroid_df.copy()\n",
    "# apply normalization techniques\n",
    "for column in norm_asteroid_df:\n",
    "    if norm_asteroid_df[column].dtype == object:\n",
    "        continue\n",
    "    norm_asteroid_df[column] = norm_asteroid_df[column] / norm_asteroid_df[column].abs().max()\n",
    "# View normalised dataset\n",
    "print(norm_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Object_Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                      2         0.807646            0.590282   \n",
      "2                      2         0.758732            0.610967   \n",
      "4                      2         0.587438            0.469295   \n",
      "8                      2         0.570204            0.427279   \n",
      "9                      0         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Edit name of column \"Object Classification\" with _\n",
    "# For using this name in next cell\n",
    "number_asteroid_df = norm_asteroid_df.copy()\n",
    "number_asteroid_df.rename(columns={\"Object Classification\": \"Object_Classification\"}, inplace=True)\n",
    "# Replace string class to numbers\n",
    "obj_class = {\"Apollo\": 0, \"Aten\": 1, \"Amor\": 2}\n",
    "number_asteroid_df.Object_Classification = [obj_class[item] for item in number_asteroid_df.Object_Classification]\n",
    "# View new dataset\n",
    "print(number_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063   1.0     0.0   0.0  \n",
      "2             0.661017            0.425397   1.0     0.0   0.0  \n",
      "4             0.450847            0.561905   1.0     0.0   0.0  \n",
      "8             0.430508            0.419048   1.0     0.0   0.0  \n",
      "9             0.271186            0.451746   0.0     1.0   0.0  \n"
     ]
    }
   ],
   "source": [
    "# One-hot Encoding the Object Classification Feature\n",
    "one_hot = OneHotEncoder()\n",
    "# Copy our dataset\n",
    "onehot_asteroid_df = norm_asteroid_df.copy()\n",
    "# Fitting one-hot encoder\n",
    "encoded = one_hot.fit_transform(onehot_asteroid_df[[\"Object Classification\"]])\n",
    "onehot_asteroid_df[one_hot.categories_[0]] = encoded.toarray()\n",
    "# Drop unnecessary \"Object Classification\" feature\n",
    "onehot_asteroid_df.drop([\"Object Classification\"], axis=1, inplace=True)\n",
    "print(onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbit Axis (AU)            float64\n",
      "Orbit Eccentricity         float64\n",
      "Orbit Inclination (deg)    float64\n",
      "Orbital Period (yr)        float64\n",
      "Asteroid Magnitude         float64\n",
      "Amor                         int32\n",
      "Apollo                       int32\n",
      "Aten                         int32\n",
      "dtype: object\n",
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063     1       0     0  \n",
      "2             0.661017            0.425397     1       0     0  \n",
      "4             0.450847            0.561905     1       0     0  \n",
      "8             0.430508            0.419048     1       0     0  \n",
      "9             0.271186            0.451746     0       1     0  \n"
     ]
    }
   ],
   "source": [
    "# Change data type in one-hot encoded column\n",
    "column_dtype_dict = {\"Amor\": int,\n",
    "                     \"Apollo\": int,\n",
    "                     \"Aten\": int}\n",
    "norm_onehot_asteroid_df = onehot_asteroid_df.astype(column_dtype_dict)\n",
    "print(norm_onehot_asteroid_df.dtypes)\n",
    "print(norm_onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Dataset to Train & Test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Split Categorical Dataset\n",
    "x = norm_asteroid_df.drop([\"Object Classification\"], axis=1)\n",
    "y = norm_asteroid_df[\"Object Classification\"]\n",
    "# Split to train test sets\n",
    "catg_X_train, catg_X_test, catg_y_train, catg_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Split Numeric Dataset\n",
    "x = number_asteroid_df.drop([\"Object_Classification\"], axis=1)\n",
    "y = number_asteroid_df[\"Object_Classification\"]\n",
    "# Split to train test sets\n",
    "num_X_train, num_X_test, num_y_train, num_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Split One-Hot Dataset\n",
    "x = norm_onehot_asteroid_df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "y = norm_onehot_asteroid_df[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "# Split to train test sets\n",
    "# oneh_X_train, oneh_X_valid, oneh_X_test = np.split(x.sample(frac=1), [int(0.8*len(x)), int(0.9*len(x))])\n",
    "# oneh_y_train, oneh_y_valid, oneh_y_test = np.split(y.sample(frac=1), [int(0.8*len(y)), int(0.9*len(y))])\n",
    "oneh_train, oneh_valid, oneh_test = np.split(norm_onehot_asteroid_df.sample(frac=1), [int(0.8*len(norm_onehot_asteroid_df)), int(0.9*len(norm_onehot_asteroid_df))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    Function for converting dataframe var to tf dataset.\n",
    "    :param dataframe:\n",
    "    :param shuffle:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Copy dataframe to new vars\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "    labels = dataframe[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "    # Convert dataframe with data to tensor with concrete dtype -- float32\n",
    "    df = tf.constant(df, dtype=tf.float32)\n",
    "    # Make TensorFlow Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df, labels))\n",
    "    # Optimize TF Dataset for loading to model\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Define batch size variable\n",
    "batch_size = 32\n",
    "# Get TensorFlow Dataset object\n",
    "train_ds = df_to_dataset(oneh_train, batch_size=batch_size)\n",
    "valid_ds = df_to_dataset(oneh_valid, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(oneh_test, shuffle=False, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 5), dtype=float32, numpy=\n",
      "array([[0.691573  , 0.74822235, 0.05346096, 0.5762712 , 0.86031747],\n",
      "       [0.74713033, 0.60881275, 0.23767088, 0.64576274, 0.6952381 ],\n",
      "       [0.61165017, 0.52908856, 0.23881587, 0.4779661 , 0.7777778 ],\n",
      "       [0.3599437 , 0.35811248, 0.3107529 , 0.21694915, 0.6920635 ],\n",
      "       [0.51602435, 0.45593622, 0.32116282, 0.37118644, 0.74920636],\n",
      "       [0.49398512, 0.4771601 , 0.10288757, 0.34745762, 0.85714287],\n",
      "       [0.34473047, 0.37319544, 0.02777941, 0.20338982, 0.768254  ],\n",
      "       [0.35721937, 0.37621203, 0.14390735, 0.21355931, 0.73968256],\n",
      "       [0.33420062, 0.5287654 , 0.13169159, 0.19322033, 0.71746033],\n",
      "       [0.5683063 , 0.38149107, 0.15756781, 0.42881355, 0.6507937 ],\n",
      "       [0.41482753, 0.6149537 , 0.16654329, 0.2677966 , 0.7714286 ],\n",
      "       [0.4875264 , 0.65513897, 0.69243616, 0.34067798, 0.7015873 ],\n",
      "       [0.51841193, 0.6356389 , 0.69989526, 0.37288135, 0.584127  ],\n",
      "       [0.33769017, 0.17420815, 0.6442519 , 0.19661017, 0.73650795],\n",
      "       [0.6810126 , 0.5727214 , 0.5112309 , 0.56271183, 0.72063494],\n",
      "       [0.76436377, 0.56453353, 0.31297627, 0.6694915 , 0.5714286 ],\n",
      "       [0.9188833 , 0.6579401 , 0.67119366, 0.88135594, 0.5460318 ],\n",
      "       [0.72086686, 0.6156001 , 0.2252963 , 0.6118644 , 0.7936508 ],\n",
      "       [0.4911996 , 0.50107735, 0.27538633, 0.34406778, 0.5936508 ],\n",
      "       [0.57901984, 0.6752855 , 0.02421637, 0.44067797, 0.75555557],\n",
      "       [0.54188985, 0.326546  , 0.79395336, 0.4       , 0.64444447],\n",
      "       [0.6080688 , 0.8165266 , 0.04215709, 0.47457626, 0.71746033],\n",
      "       [0.6171294 , 0.856281  , 0.09354324, 0.48474577, 0.8       ],\n",
      "       [0.50583124, 0.5985779 , 0.7416143 , 0.35932204, 0.6698413 ],\n",
      "       [0.52667665, 0.38181427, 0.90027076, 0.38305086, 0.584127  ],\n",
      "       [0.478374  , 0.2340013 , 0.54686385, 0.33050847, 0.64761907],\n",
      "       [0.4604671 , 0.40088344, 0.440695  , 0.31355932, 0.6603175 ],\n",
      "       [0.86047935, 0.679164  , 0.04443426, 0.7983051 , 0.7746032 ],\n",
      "       [0.5658575 , 0.4154277 , 0.5599531 , 0.42542374, 0.6603175 ],\n",
      "       [0.41403165, 0.13725491, 0.5191203 , 0.2661017 , 0.63809526],\n",
      "       [0.39539012, 0.5279035 , 0.1425472 , 0.24915254, 0.83492064],\n",
      "       [0.7672411 , 0.63520795, 0.30668524, 0.67288136, 0.784127  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(32, 3), dtype=int32, numpy=\n",
      "array([[1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1]])>)\n"
     ]
    }
   ],
   "source": [
    "# View new train dataset\n",
    "for onehot in train_ds.take(1):\n",
    "    print(onehot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def model_compile(model, loss_func, learn_rate=0.001):\n",
    "    \"\"\"\n",
    "    Help function for compiling model;\n",
    "    :param model: built model;\n",
    "    :param loss_func: loss function for compiling;\n",
    "    :param learn_rate: learning rate for optimizer.\n",
    "    \"\"\"\n",
    "    model.compile(loss=loss_func,\n",
    "                  optimizer=Adam(learning_rate=learn_rate),\n",
    "                  metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Set Input layer for input data\n",
    "input_shape = len(asteroid_features)-1\n",
    "inputs = Input(shape=(input_shape,), name=\"model_inputs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Set Dense (Fully Connected) layer with 100 hidden unit and \"relu\" activation function\n",
    "x = Dense(100, activation=\"relu\")(inputs)\n",
    "# Set another Dense layer with 10 hidden unit and \"relu\" activation function\n",
    "x = Dense(10, activation=\"relu\")(x)\n",
    "# Set output layer with num_classes hidden unit and \"sigmoid\" activation function\n",
    "outputs = Dense(num_classes, activation=\"sigmoid\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Define our model\n",
    "onehot_model = Model(inputs, outputs, name=\"onehot_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"onehot_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_inputs (InputLayer)   [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               600       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,643\n",
      "Trainable params: 1,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View summary of the model\n",
    "onehot_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(onehot_model, CategoricalCrossentropy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "333/333 [==============================] - 3s 4ms/step - loss: 0.7306 - accuracy: 0.6950 - val_loss: 0.5407 - val_accuracy: 0.7917\n",
      "Epoch 2/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.4504 - accuracy: 0.8159 - val_loss: 0.3928 - val_accuracy: 0.8098\n",
      "Epoch 3/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.3297 - accuracy: 0.8551 - val_loss: 0.2794 - val_accuracy: 0.9038\n",
      "Epoch 4/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.2540 - accuracy: 0.9100 - val_loss: 0.2196 - val_accuracy: 0.9308\n",
      "Epoch 5/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.2061 - accuracy: 0.9354 - val_loss: 0.1861 - val_accuracy: 0.9338\n",
      "Epoch 6/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1825 - accuracy: 0.9387 - val_loss: 0.1567 - val_accuracy: 0.9549\n",
      "Epoch 7/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1591 - accuracy: 0.9492 - val_loss: 0.1404 - val_accuracy: 0.9624\n",
      "Epoch 8/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.9509 - val_loss: 0.1396 - val_accuracy: 0.9504\n",
      "Epoch 9/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.9524 - val_loss: 0.1197 - val_accuracy: 0.9639\n",
      "Epoch 10/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1278 - accuracy: 0.9579 - val_loss: 0.1168 - val_accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "onehot_history = onehot_model.fit(train_ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10,\n",
    "                                  validation_data=valid_ds,\n",
    "                                  use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9609\n",
      "Model loss on the test set: 0.12188560515642166\n",
      "Model accuracy on the test set = 0.960931658744812\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = onehot_model.evaluate(test_ds)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set = {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last best result on this data is that:\n",
    "**model** - GridSearch(SVM)\n",
    "**result** - accuracy = 0.99\n",
    "\n",
    "Now result is\n",
    "**model** - Fully connected feed-forward network\n",
    "**result** - accuracy = 0.96\n",
    "\n",
    "In summary, we can say model (GridSearch(SVM)) have best fit on this data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Food Classification Dataset (from [kaggle]())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def prepare_ds(dataset, shuffle=True, batch=32):\n",
    "    \"\"\"\n",
    "    Help function for prepare dataset for better performance;\n",
    "    :param dataset: TensorFlow Dataset object;\n",
    "    :param shuffle: bool for define make shuffle or not;\n",
    "    :param batch: size of batch in dataset;\n",
    "    :return: normalized and prepared dataset.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        dataset.shuffle(buffer_size=len(dataset))\n",
    "    dataset.batch(batch)\n",
    "    dataset.prefetch(batch)\n",
    "    norm_layer = Rescaling(1./255)\n",
    "    norm_ds = dataset.map(lambda x, y: (norm_layer(x), y))\n",
    "    image_batch, labels_batch = next(iter(norm_ds))\n",
    "    first_image = image_batch[0]\n",
    "    print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "    return norm_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Define train and test dir\n",
    "train_dir = pathlib.Path(\"datasets\\\\10_food_classes_all_data\\\\train\\\\\")\n",
    "test_dir = pathlib.Path(\"datasets\\\\10_food_classes_all_data\\\\test\\\\\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('datasets/10_food_classes_all_data/train')"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 files belonging to 10 classes.\n",
      "Using 6000 files for training.\n",
      "Found 7500 files belonging to 10 classes.\n",
      "Using 1500 files for validation.\n",
      "Found 2500 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define some important variable\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "random_seed = 17\n",
    "# Set train and test datasets\n",
    "train_ds = image_dataset_from_directory(train_dir,\n",
    "                                        label_mode=\"int\",\n",
    "                                        validation_split=0.2,\n",
    "                                        subset=\"training\",\n",
    "                                        seed=random_seed,\n",
    "                                        image_size=image_size,\n",
    "                                        batch_size=batch_size)\n",
    "val_ds = image_dataset_from_directory(train_dir,\n",
    "                                      label_mode=\"int\",\n",
    "                                      validation_split=0.2,\n",
    "                                      subset=\"validation\",\n",
    "                                      seed=random_seed,\n",
    "                                      image_size=image_size,\n",
    "                                      batch_size=batch_size)\n",
    "test_ds = image_dataset_from_directory(test_dir,\n",
    "                                       label_mode=\"int\",\n",
    "                                       seed=random_seed,\n",
    "                                       image_size=image_size,\n",
    "                                       batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View on train Dataset\n",
    "train_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(32, 224, 224, 3), dtype=float32, numpy=\n array([[[[  9.        ,  12.        ,  27.        ],\n          [  9.        ,  12.        ,  27.        ],\n          [  9.        ,  12.        ,  27.        ],\n          ...,\n          [  9.642858  ,  11.642858  ,  26.642857  ],\n          [  9.642858  ,  11.642858  ,  26.642857  ],\n          [  9.642858  ,  11.642858  ,  26.642857  ]],\n \n         [[  9.        ,  12.        ,  27.        ],\n          [  9.        ,  12.        ,  27.        ],\n          [  9.        ,  12.        ,  27.        ],\n          ...,\n          [  8.071428  ,  10.071428  ,  23.071428  ],\n          [  8.071428  ,  10.071428  ,  23.071428  ],\n          [  8.071428  ,  10.071428  ,  23.071428  ]],\n \n         [[  9.214286  ,  12.        ,  26.571428  ],\n          [  9.214286  ,  12.        ,  26.571428  ],\n          [  9.214286  ,  12.        ,  26.571428  ],\n          ...,\n          [ 10.785714  ,  12.785714  ,  25.571428  ],\n          [ 10.785714  ,  12.785714  ,  25.571428  ],\n          [ 10.785714  ,  12.785714  ,  25.571428  ]],\n \n         ...,\n \n         [[ 37.95415   ,  41.95415   ,  42.95415   ],\n          [ 30.071648  ,  34.071648  ,  35.071648  ],\n          [ 36.571472  ,  40.571472  ,  41.571472  ],\n          ...,\n          [ 35.04591   ,  39.04591   ,  40.04591   ],\n          [ 32.428772  ,  36.428772  ,  37.428772  ],\n          [ 38.872627  ,  42.872627  ,  43.872627  ]],\n \n         [[ 81.7983    ,  85.75237   ,  88.58908   ],\n          [ 87.578835  ,  91.57373   ,  94.359406  ],\n          [ 81.90544   ,  85.90544   ,  88.72173   ],\n          ...,\n          [ 81.54833   ,  85.54833   ,  88.390144  ],\n          [ 87.578835  ,  91.50739   ,  94.29307   ],\n          [ 80.65022   ,  84.65022   ,  87.46141   ]],\n \n         [[244.77046   , 248.77046   , 251.77046   ],\n          [240.44911   , 244.44911   , 247.44911   ],\n          [244.48488   , 248.48488   , 251.20424   ],\n          ...,\n          [242.9083    , 246.9083    , 249.9083    ],\n          [244.16347   , 248.16347   , 251.16347   ],\n          [243.94395   , 247.94395   , 250.94395   ]]],\n \n \n        [[[  4.        ,   4.        ,   4.        ],\n          [  3.        ,   3.        ,   3.        ],\n          [  3.        ,   3.        ,   1.        ],\n          ...,\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ]],\n \n         [[  4.        ,   4.        ,   4.        ],\n          [  3.        ,   3.        ,   3.        ],\n          [  3.        ,   3.        ,   1.        ],\n          ...,\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ]],\n \n         [[  4.        ,   4.        ,   4.        ],\n          [  3.        ,   3.        ,   3.        ],\n          [  3.        ,   3.        ,   1.        ],\n          ...,\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ],\n          [  1.        ,   1.        ,   1.        ]],\n \n         ...,\n \n         [[217.352     , 216.352     , 212.352     ],\n          [218.21426   , 217.21426   , 213.21426   ],\n          [219.21426   , 218.21426   , 214.21426   ],\n          ...,\n          [135.21426   , 130.21426   , 124.214264  ],\n          [135.        , 130.        , 124.        ],\n          [133.64282   , 128.64282   , 122.64282   ]],\n \n         [[215.92856   , 214.92856   , 210.92856   ],\n          [216.92856   , 215.92856   , 211.92856   ],\n          [217.92856   , 216.92856   , 212.92856   ],\n          ...,\n          [136.22957   , 131.22957   , 125.22957   ],\n          [135.06635   , 130.06635   , 124.06634   ],\n          [133.28564   , 128.28564   , 122.285645  ]],\n \n         [[213.64282   , 212.64282   , 208.64282   ],\n          [214.5714    , 213.5714    , 209.5714    ],\n          [215.64282   , 214.64282   , 210.64282   ],\n          ...,\n          [137.21426   , 132.21426   , 126.214264  ],\n          [135.92856   , 130.92856   , 124.92856   ],\n          [133.28564   , 128.28564   , 122.285645  ]]],\n \n \n        [[[204.77042   , 218.77042   , 244.77042   ],\n          [204.2143    , 218.2143    , 244.2143    ],\n          [210.        , 224.42857   , 251.21428   ],\n          ...,\n          [186.        , 203.        , 210.78574   ],\n          [187.07144   , 204.07144   , 212.07144   ],\n          [188.58676   , 205.58676   , 213.58676   ]],\n \n         [[203.73979   , 217.73979   , 243.73979   ],\n          [206.        , 220.        , 246.        ],\n          [205.62755   , 220.05612   , 246.05612   ],\n          ...,\n          [187.05612   , 202.05612   , 209.05612   ],\n          [188.13777   , 202.13777   , 211.13777   ],\n          [189.40309   , 203.40309   , 212.40309   ]],\n \n         [[205.71939   , 219.93367   , 245.29082   ],\n          [206.78572   , 220.78572   , 246.78572   ],\n          [206.78572   , 221.21428   , 247.        ],\n          ...,\n          [186.95409   , 199.95409   , 207.86226   ],\n          [188.07144   , 201.07144   , 209.07144   ],\n          [189.28064   , 202.28064   , 211.06636   ]],\n \n         ...,\n \n         [[187.        , 199.        , 211.        ],\n          [188.92857   , 200.92857   , 213.3571    ],\n          [188.83163   , 200.83163   , 214.83163   ],\n          ...,\n          [186.38261   , 202.59688   , 224.95409   ],\n          [185.9847    , 202.19896   , 224.55617   ],\n          [184.92348   , 201.13774   , 223.49495   ]],\n \n         [[188.        , 200.        , 212.        ],\n          [188.93367   , 200.93367   , 212.93367   ],\n          [187.9847    , 199.9847    , 212.41327   ],\n          ...,\n          [186.0153    , 202.0153    , 225.0153    ],\n          [185.06635   , 201.06635   , 224.06635   ],\n          [186.33167   , 202.33167   , 225.33167   ]],\n \n         [[188.77042   , 200.77042   , 212.77042   ],\n          [189.        , 201.        , 213.        ],\n          [187.78572   , 199.78572   , 211.78572   ],\n          ...,\n          [186.70918   , 202.70918   , 225.70918   ],\n          [184.04593   , 200.04593   , 223.04593   ],\n          [185.2296    , 201.2296    , 224.2296    ]]],\n \n \n        ...,\n \n \n        [[[ 54.688774  ,  46.561222  ,  32.15816   ],\n          [ 49.44898   ,  39.239796  ,  23.836735  ],\n          [ 52.632652  ,  39.0051    ,  23.005102  ],\n          ...,\n          [118.35713   , 108.57143   ,  80.71425   ],\n          [118.500015  , 110.428604  ,  74.78566   ],\n          [124.08672   , 116.08672   ,  77.80102   ]],\n \n         [[ 42.826527  ,  38.68367   ,  28.806122  ],\n          [ 46.719387  ,  39.714287  ,  29.5       ],\n          [ 44.02551   ,  36.311226  ,  25.02551   ],\n          ...,\n          [121.785706  , 115.72958   ,  83.89795   ],\n          [120.13774   , 115.86226   ,  81.14282   ],\n          [121.56633   , 116.806114  ,  81.97449   ]],\n \n         [[ 35.040813  ,  31.612246  ,  26.964287  ],\n          [ 41.392857  ,  36.392857  ,  32.25      ],\n          [ 42.19388   ,  35.62245   ,  28.051022  ],\n          ...,\n          [119.33674   , 116.4745    ,  81.85717   ],\n          [118.12752   , 117.42857   ,  86.071465  ],\n          [117.99491   , 116.571434  ,  86.71939   ]],\n \n         ...,\n \n         [[106.78567   , 101.280556  ,  73.71938   ],\n          [107.41322   , 102.41322   ,  73.19896   ],\n          [109.2347    , 104.5714    ,  75.18879   ],\n          ...,\n          [110.06637   , 103.4694    ,  69.545944  ],\n          [105.68357   ,  97.22954   ,  61.397865  ],\n          [105.9285    ,  97.71423   ,  61.07144   ]],\n \n         [[109.16833   , 101.16833   ,  79.50508   ],\n          [110.92856   , 102.99488   ,  79.92858   ],\n          [112.66838   , 105.42854   ,  79.699     ],\n          ...,\n          [115.3877    , 106.26013   ,  75.775475  ],\n          [109.35195   , 102.20403   ,  74.275505  ],\n          [108.52548   , 102.193855  ,  75.76533   ]],\n \n         [[103.484634  ,  95.484634  ,  76.025475  ],\n          [110.765274  ,  99.765274  ,  79.67345   ],\n          [114.71432   , 103.71432   ,  81.86739   ],\n          ...,\n          [111.862305  , 101.0766    ,  71.87257   ],\n          [110.92859   , 103.44911   ,  80.54615   ],\n          [105.199005  ,  99.48471   ,  80.07147   ]]],\n \n \n        [[[ 16.67347   ,  31.67347   ,  38.67347   ],\n          [ 17.90306   ,  32.903065  ,  39.903065  ],\n          [ 15.64796   ,  30.64796   ,  37.64796   ],\n          ...,\n          [129.84691   , 161.27544   , 190.70396   ],\n          [126.357155  , 157.35716   , 186.35716   ],\n          [129.35721   , 160.35721   , 188.35721   ]],\n \n         [[ 17.051018  ,  32.051018  ,  39.05102   ],\n          [ 18.729593  ,  33.72959   ,  40.72959   ],\n          [ 16.459185  ,  31.459185  ,  38.459183  ],\n          ...,\n          [130.2755    , 161.2755    , 190.70403   ],\n          [130.9949    , 159.9949    , 189.9949    ],\n          [133.31126   , 162.31126   , 192.31126   ]],\n \n         [[  8.168368  ,  23.168367  ,  29.739796  ],\n          [ 14.632654  ,  29.632654  ,  36.204082  ],\n          [ 21.44898   ,  35.68367   ,  42.637753  ],\n          ...,\n          [129.44905   , 155.61737   , 188.70921   ],\n          [132.51529   , 159.08673   , 189.30101   ],\n          [130.71936   , 157.29079   , 187.50508   ]],\n \n         ...,\n \n         [[ 18.128054  ,  23.561785  ,  19.699526  ],\n          [ 14.255497  ,  19.612705  ,  15.826969  ],\n          [ 13.974876  ,  19.974876  ,  15.974876  ],\n          ...,\n          [248.75511   , 247.80101   , 246.48972   ],\n          [244.65323   , 233.39299   , 208.75014   ],\n          [171.56541   , 152.2848    , 102.57053   ]],\n \n         [[  1.408421  ,   5.882932  ,   1.8829322 ],\n          [  1.219611  ,   6.224727  ,   2.2247272 ],\n          [  1.5409544 ,   7.5409546 ,   3.5409544 ],\n          ...,\n          [245.4847    , 248.301     , 248.09682   ],\n          [225.56622   , 219.76523   , 203.27025   ],\n          [223.23499   , 210.13805   , 175.42372   ]],\n \n         [[  9.968994  ,  15.968994  ,  11.968994  ],\n          [ 11.433054  ,  17.101389  ,  13.101389  ],\n          [ 15.784912  ,  21.784912  ,  17.784912  ],\n          ...,\n          [240.76031   , 247.55617   , 243.54083   ],\n          [243.24925   , 243.53998   , 229.84608   ],\n          [154.71384   , 147.25462   , 121.943436  ]]],\n \n \n        [[[ 75.12755   , 115.99681   , 144.06218   ],\n          [ 76.80198   , 117.80198   , 147.80197   ],\n          [ 72.29146   , 109.963646  , 143.01085   ],\n          ...,\n          [ 57.334114  ,  65.69132   ,  64.90559   ],\n          [ 50.315647  ,  59.315647  ,  56.315647  ],\n          [ 18.667942  ,  28.792538  ,  24.792538  ]],\n \n         [[ 77.77168   , 116.76308   , 146.76753   ],\n          [ 78.64349   , 118.54528   , 150.1926    ],\n          [ 76.182076  , 111.75351   , 147.06601   ],\n          ...,\n          [ 35.38913   ,  43.746338  ,  42.9606    ],\n          [ 42.8808    ,  51.8808    ,  49.97901   ],\n          [ 42.981323  ,  52.88311   ,  49.432217  ]],\n \n         [[ 76.4662    , 111.71461   , 143.00638   ],\n          [ 76.11065   , 112.36511   , 146.14955   ],\n          [ 84.05868   , 118.19196   , 155.32143   ],\n          ...,\n          [  1.6358528 ,   8.489613  ,   7.7038765 ],\n          [  6.658497  ,  15.658497  ,  14.658497  ],\n          [ 17.402327  ,  26.402327  ,  23.402327  ]],\n \n         ...,\n \n         [[166.71518   , 190.71518   , 218.60611   ],\n          [170.59528   , 191.59528   , 220.43771   ],\n          [170.46454   , 191.46454   , 220.33122   ],\n          ...,\n          [ 12.2242775 ,  15.187922  ,  19.456394  ],\n          [  9.7844305 ,  12.680134  ,  17.294855  ],\n          [ 13.463774  ,  16.620972  ,  20.712704  ]],\n \n         [[158.23209   , 181.16927   , 207.14279   ],\n          [160.68744   , 182.10612   , 207.9285    ],\n          [160.88068   , 182.13834   , 208.31084   ],\n          ...,\n          [  1.4171449 ,   3.2800446 ,   4.6343307 ],\n          [  5.77966   ,   6.264337  ,   7.3947554 ],\n          [ 68.18368   ,  67.50888   ,  69.05798   ]],\n \n         [[138.04587   , 159.98051   , 184.1766    ],\n          [139.11124   , 161.09816   , 185.13737   ],\n          [139.71487   , 161.2863    , 185.57903   ],\n          ...,\n          [  0.57520247,   1.4584191 ,   3.2441552 ],\n          [  4.191408  ,   2.5896533 ,   3.3613474 ],\n          [ 63.684364  ,  58.285683  ,  60.10267   ]]]], dtype=float32)>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([9, 4, 2, 0, 9, 7, 2, 5, 7, 2, 9, 8, 7, 0, 3, 6, 3, 9, 3, 0, 2, 9,\n        9, 8, 9, 3, 3, 6, 8, 3, 9, 3])>)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View on test Dataset\n",
    "next(iter(train_ds))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "['chicken_curry',\n 'chicken_wings',\n 'fried_rice',\n 'grilled_salmon',\n 'hamburger',\n 'ice_cream',\n 'pizza',\n 'ramen',\n 'steak',\n 'sushi']"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# View number of classes\n",
    "num_image_classes = len(train_ds.class_names)\n",
    "print(num_image_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.8341549\n",
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for better performance\n",
    "train_ds = prepare_ds(train_ds)\n",
    "val_ds = prepare_ds(val_ds)\n",
    "test_ds = prepare_ds(test_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape = (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set Input layer for input data\n",
    "image_shape = None\n",
    "for images in train_ds.take(1):\n",
    "    image_shape = images[0].shape[1:]\n",
    "print(f\"image shape = {image_shape}\")\n",
    "inputs = Input(shape=image_shape, name=\"model_inputs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Set Convolution layers\n",
    "x = Conv2D(10, 3, activation=\"relu\")(inputs)\n",
    "x = Conv2D(10, 3, activation=\"relu\")(x)\n",
    "# Set Max Pooling 2D layer\n",
    "x = MaxPool2D()(x)\n",
    "# Set another Convolution layers\n",
    "x = Conv2D(10, 3, activation=\"relu\")(x)\n",
    "x = Conv2D(10, 3, activation=\"relu\")(x)\n",
    "# Set another Max Pooling 2D layer\n",
    "x = MaxPool2D()(x)\n",
    "# Set Flatten and finish Dense (Fully connected) layers\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(num_image_classes, activation=\"softmax\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# Define model object\n",
    "conv_model = Model(inputs, outputs, name=\"convolution_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(conv_model, SparseCategoricalCrossentropy(), learn_rate=0.0001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 18s 88ms/step - loss: 2.2160 - accuracy: 0.1808 - val_loss: 2.1406 - val_accuracy: 0.2093\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 17s 87ms/step - loss: 2.0303 - accuracy: 0.2778 - val_loss: 2.0169 - val_accuracy: 0.2787\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 1.8863 - accuracy: 0.3487 - val_loss: 1.9372 - val_accuracy: 0.3160\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 18s 93ms/step - loss: 1.7710 - accuracy: 0.4008 - val_loss: 1.9304 - val_accuracy: 0.3300\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 17s 88ms/step - loss: 1.6830 - accuracy: 0.4328 - val_loss: 1.9332 - val_accuracy: 0.3260\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 1.5791 - accuracy: 0.4745 - val_loss: 1.9405 - val_accuracy: 0.3267\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 1.4743 - accuracy: 0.5117 - val_loss: 2.0088 - val_accuracy: 0.3267\n",
      "Epoch 8/10\n",
      " 88/188 [=============>................] - ETA: 8s - loss: 1.3610 - accuracy: 0.5732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit model with train and test data\n",
    "conv_history = conv_model.fit(train_ds,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_ds,\n",
    "                              use_multiprocessing=True\n",
    "                              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 7s 89ms/step - loss: 5.7694 - accuracy: 0.1672\n",
      "Model loss on the test set: 5.769413471221924\n",
      "Model accuracy on the test set: 0.1671999990940094\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = conv_model.evaluate(test_ds)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In summary, we have that result:\n",
    "**model** -- Convolution Neural Network (CNN)\n",
    "**result** -- accuracy = ; loss = ;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Emotion Detection from Text (from [kaggle](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
