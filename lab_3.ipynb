{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laboratory work #3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all significant libraries for this project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow & Keras Libraries\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers import Conv2D, RNN\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "# Import scikit-learn libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import libraries for text cleaning\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Who\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Possible Asteroid Impacts with Earth (from [kaggle](https://www.kaggle.com/datasets/nasa/asteroid-impacts))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Loading dataset to pandas DataFrame\n",
    "asteroid_df = pd.read_csv(\"datasets\\\\asteroid_dataset\\\\asteroid_classification.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n",
      "       'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for analysis from the dataframe\n",
    "asteroid_df.drop([\"Object Name\", \"Epoch (TDB)\", \"Perihelion Argument (deg)\", \"Node Longitude (deg)\",\n",
    "                  \"Mean Anomoly (deg)\", \"Perihelion Distance (AU)\", \"Aphelion Distance (AU)\",\n",
    "                  \"Minimum Orbit Intersection Distance (AU)\", \"Orbital Reference\"], axis=1, inplace=True)\n",
    "print(asteroid_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15634 entries, 0 to 15634\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Object Classification    15634 non-null  object \n",
      " 1   Orbit Axis (AU)          15634 non-null  float64\n",
      " 2   Orbit Eccentricity       15634 non-null  float64\n",
      " 3   Orbit Inclination (deg)  15634 non-null  float64\n",
      " 4   Orbital Period (yr)      15634 non-null  float64\n",
      " 5   Asteroid Magnitude       15634 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 855.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop null values of dataframe as we have only one null value\n",
    "asteroid_df.dropna(inplace=True)\n",
    "asteroid_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Change class names\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apollo Asteroid\" ,\n",
    "                                          \"Apollo\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Aten Asteroid\",\n",
    "                                          \"Aten\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Amor Asteroid\",\n",
    "                                          \"Amor\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apohele Asteroid\",\n",
    "                                          \"Apohele\", inplace=True)\n",
    "# Drop unnecessary class\n",
    "necessary_class = [\"Apollo\", \"Aten\", \"Amor\"]\n",
    "asteroid_df = asteroid_df[asteroid_df[\"Object Classification\"].isin(necessary_class)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, features):\n",
    "    \"\"\"Function to remove the outliers;\n",
    "    :param dataframe: pandas DataFrame with data;\n",
    "    :param features: list with features from dataframe.\n",
    "    \"\"\"\n",
    "    # Copy dataframe to another variable\n",
    "    dataframe_copy = dataframe.copy()\n",
    "\n",
    "    # Iterate through features\n",
    "    for feature in features:\n",
    "        if dataframe[feature].dtype == object:\n",
    "            continue\n",
    "        # Calculate q1, q3 and iqr\n",
    "        q3 = dataframe[feature].quantile(0.75)\n",
    "        q1 = dataframe[feature].quantile(0.25)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Get local minimum and maximum\n",
    "        local_min = q1 - (1.5 * iqr)\n",
    "        local_max = q3 + (1.5 * iqr)\n",
    "\n",
    "        # Remove the outliers\n",
    "        dataframe_copy = dataframe_copy[(dataframe_copy[feature] >= local_min) &\n",
    "                                        (dataframe_copy[feature] <= local_max)]\n",
    "\n",
    "    return dataframe_copy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the df\n",
    "asteroid_features = asteroid_df.columns.tolist()\n",
    "print(asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Remove outliers from the dataframe\n",
    "asteroid_df = remove_outliers(asteroid_df, asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apollo    6651\n",
      "Amor      5686\n",
      "Aten       965\n",
      "Name: Object Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View count of class names\n",
    "print(asteroid_df.iloc[:, 0].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "num_classes = len(necessary_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Object Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                  Amor         0.807646            0.590282   \n",
      "2                  Amor         0.758732            0.610967   \n",
      "4                  Amor         0.587438            0.469295   \n",
      "8                  Amor         0.570204            0.427279   \n",
      "9                Apollo         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Normalise dataset\n",
    "norm_asteroid_df = asteroid_df.copy()\n",
    "# apply normalization techniques\n",
    "for column in norm_asteroid_df:\n",
    "    if norm_asteroid_df[column].dtype == object:\n",
    "        continue\n",
    "    norm_asteroid_df[column] = norm_asteroid_df[column] / norm_asteroid_df[column].abs().max()\n",
    "# View normalised dataset\n",
    "print(norm_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Object_Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                      2         0.807646            0.590282   \n",
      "2                      2         0.758732            0.610967   \n",
      "4                      2         0.587438            0.469295   \n",
      "8                      2         0.570204            0.427279   \n",
      "9                      0         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Edit name of column \"Object Classification\" with _\n",
    "# For using this name in next cell\n",
    "number_asteroid_df = norm_asteroid_df.copy()\n",
    "number_asteroid_df.rename(columns={\"Object Classification\": \"Object_Classification\"}, inplace=True)\n",
    "# Replace string class to numbers\n",
    "obj_class = {\"Apollo\": 0, \"Aten\": 1, \"Amor\": 2}\n",
    "number_asteroid_df.Object_Classification = [obj_class[item] for item in number_asteroid_df.Object_Classification]\n",
    "# View new dataset\n",
    "print(number_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063   1.0     0.0   0.0  \n",
      "2             0.661017            0.425397   1.0     0.0   0.0  \n",
      "4             0.450847            0.561905   1.0     0.0   0.0  \n",
      "8             0.430508            0.419048   1.0     0.0   0.0  \n",
      "9             0.271186            0.451746   0.0     1.0   0.0  \n"
     ]
    }
   ],
   "source": [
    "# One-hot Encoding the Object Classification Feature\n",
    "one_hot = OneHotEncoder()\n",
    "# Copy our dataset\n",
    "onehot_asteroid_df = norm_asteroid_df.copy()\n",
    "# Fitting one-hot encoder\n",
    "encoded = one_hot.fit_transform(onehot_asteroid_df[[\"Object Classification\"]])\n",
    "onehot_asteroid_df[one_hot.categories_[0]] = encoded.toarray()\n",
    "# Drop unnecessary \"Object Classification\" feature\n",
    "onehot_asteroid_df.drop([\"Object Classification\"], axis=1, inplace=True)\n",
    "print(onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbit Axis (AU)            float64\n",
      "Orbit Eccentricity         float64\n",
      "Orbit Inclination (deg)    float64\n",
      "Orbital Period (yr)        float64\n",
      "Asteroid Magnitude         float64\n",
      "Amor                         int32\n",
      "Apollo                       int32\n",
      "Aten                         int32\n",
      "dtype: object\n",
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063     1       0     0  \n",
      "2             0.661017            0.425397     1       0     0  \n",
      "4             0.450847            0.561905     1       0     0  \n",
      "8             0.430508            0.419048     1       0     0  \n",
      "9             0.271186            0.451746     0       1     0  \n"
     ]
    }
   ],
   "source": [
    "# Change data type in one-hot encoded column\n",
    "column_dtype_dict = {\"Amor\": int,\n",
    "                     \"Apollo\": int,\n",
    "                     \"Aten\": int}\n",
    "norm_onehot_asteroid_df = onehot_asteroid_df.astype(column_dtype_dict)\n",
    "print(norm_onehot_asteroid_df.dtypes)\n",
    "print(norm_onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Dataset to Train & Test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Split Categorical Dataset\n",
    "x = norm_asteroid_df.drop([\"Object Classification\"], axis=1)\n",
    "y = norm_asteroid_df[\"Object Classification\"]\n",
    "# Split to train test sets\n",
    "catg_X_train, catg_X_test, catg_y_train, catg_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Split Numeric Dataset\n",
    "x = number_asteroid_df.drop([\"Object_Classification\"], axis=1)\n",
    "y = number_asteroid_df[\"Object_Classification\"]\n",
    "# Split to train test sets\n",
    "num_X_train, num_X_test, num_y_train, num_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Split One-Hot Dataset\n",
    "x = norm_onehot_asteroid_df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "y = norm_onehot_asteroid_df[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "# Split to train test sets\n",
    "# oneh_X_train, oneh_X_valid, oneh_X_test = np.split(x.sample(frac=1), [int(0.8*len(x)), int(0.9*len(x))])\n",
    "# oneh_y_train, oneh_y_valid, oneh_y_test = np.split(y.sample(frac=1), [int(0.8*len(y)), int(0.9*len(y))])\n",
    "oneh_train, oneh_valid, oneh_test = np.split(norm_onehot_asteroid_df.sample(frac=1), [int(0.8*len(norm_onehot_asteroid_df)), int(0.9*len(norm_onehot_asteroid_df))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    Function for converting dataframe var to tf dataset.\n",
    "    :param dataframe:\n",
    "    :param shuffle:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "    labels = dataframe.pop([\"Apollo\", \"Aten\", \"Amor\"])\n",
    "    df = {key: value[:, tf.newaxis] for key, value in df.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "['Apollo', 'Aten', 'Amor']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3799\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3800\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:144\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: '['Apollo', 'Aten', 'Amor']' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [26], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Get TensorFlow Dataset object\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m \u001B[43mdf_to_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43moneh_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m valid_ds \u001B[38;5;241m=\u001B[39m df_to_dataset(oneh_valid, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[0;32m      6\u001B[0m test_ds \u001B[38;5;241m=\u001B[39m df_to_dataset(oneh_test, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n",
      "Cell \u001B[1;32mIn [25], line 10\u001B[0m, in \u001B[0;36mdf_to_dataset\u001B[1;34m(dataframe, shuffle, batch_size)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mFunction for converting dataframe var to tf dataset.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m:param dataframe:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m df \u001B[38;5;241m=\u001B[39m dataframe\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m---> 10\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mdataframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mApollo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAten\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAmor\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m df \u001B[38;5;241m=\u001B[39m {key: value[:, tf\u001B[38;5;241m.\u001B[39mnewaxis] \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     12\u001B[0m ds \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices((\u001B[38;5;28mdict\u001B[39m(df), labels))\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\frame.py:5677\u001B[0m, in \u001B[0;36mDataFrame.pop\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m   5636\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpop\u001B[39m(\u001B[38;5;28mself\u001B[39m, item: Hashable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[0;32m   5637\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   5638\u001B[0m \u001B[38;5;124;03m    Return item and drop from frame. Raise KeyError if not found.\u001B[39;00m\n\u001B[0;32m   5639\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5675\u001B[0m \u001B[38;5;124;03m    3  monkey        NaN\u001B[39;00m\n\u001B[0;32m   5676\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 5677\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\generic.py:923\u001B[0m, in \u001B[0;36mNDFrame.pop\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    921\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpop\u001B[39m(\u001B[38;5;28mself\u001B[39m, item: Hashable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series \u001B[38;5;241m|\u001B[39m Any:\n\u001B[0;32m    922\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m[item]\n\u001B[1;32m--> 923\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m[item]\n\u001B[0;32m    925\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4248\u001B[0m, in \u001B[0;36mNDFrame.__delitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4243\u001B[0m             deleted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   4244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m deleted:\n\u001B[0;32m   4245\u001B[0m     \u001B[38;5;66;03m# If the above loop ran and didn't delete anything because\u001B[39;00m\n\u001B[0;32m   4246\u001B[0m     \u001B[38;5;66;03m# there was no match, this call should raise the appropriate\u001B[39;00m\n\u001B[0;32m   4247\u001B[0m     \u001B[38;5;66;03m# exception:\u001B[39;00m\n\u001B[1;32m-> 4248\u001B[0m     loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39midelete(loc)\n\u001B[0;32m   4251\u001B[0m \u001B[38;5;66;03m# delete from the caches\u001B[39;00m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3807\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3803\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3804\u001B[0m         \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m         \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3806\u001B[0m         \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m-> 3807\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_indexing_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m   3810\u001B[0m \u001B[38;5;66;03m# GH#42269\u001B[39;00m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5963\u001B[0m, in \u001B[0;36mIndex._check_indexing_error\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   5959\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_indexing_error\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m   5960\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scalar(key):\n\u001B[0;32m   5961\u001B[0m         \u001B[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001B[39;00m\n\u001B[0;32m   5962\u001B[0m         \u001B[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001B[39;00m\n\u001B[1;32m-> 5963\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m: ['Apollo', 'Aten', 'Amor']"
     ]
    }
   ],
   "source": [
    "# Define batch size variable\n",
    "batch_size = 32\n",
    "# Get TensorFlow Dataset object\n",
    "train_ds = df_to_dataset(oneh_train, batch_size=batch_size)\n",
    "valid_ds = df_to_dataset(oneh_valid, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(oneh_test, shuffle=False, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_compile(model, learn_rate=0.001):\n",
    "    \"\"\"\n",
    "    Help function for compiling model;\n",
    "    :param model: built model;\n",
    "    :param learn_rate: learning rate for optimizer;\n",
    "    \"\"\"\n",
    "    model.compile(loss=CategoricalCrossentropy,\n",
    "                  optimizer=Adam(learning_rate=learn_rate),\n",
    "                  metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set Input layer for input data\n",
    "input_shape = len(asteroid_features)-1\n",
    "inputs = Input(shape=(input_shape,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set Dense (Fully Connected) layer with 100 hidden unit and \"relu\" activation function\n",
    "x = Dense(100, activation=\"relu\")(inputs)\n",
    "# Set another Dense layer with 10 hidden unit and \"relu\" activation function\n",
    "x = Dense(10, activation=\"relu\")(x)\n",
    "# Set output layer with num_classes hidden unit and \"sigmoid\" activation function\n",
    "outputs = Dense(num_classes, activation=\"sigmoid\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define our model\n",
    "onehot_model = Model(inputs, outputs, name=\"onehot_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# View summary of the model\n",
    "onehot_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(onehot_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "onehot_history = onehot_model.fit(train_ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10,\n",
    "                                  validation_data=valid_ds,\n",
    "                                  use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\losses.py\", line 158, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.CategoricalCrossentropy object at 0x0000026144355600> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [37], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m \u001B[43monehot_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43moneh_X_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moneh_y_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loss on the test set: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel accuracy on the test set = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileookg6aid.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\losses.py\", line 158, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.CategoricalCrossentropy object at 0x0000026144355600> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = onehot_model.evaluate(oneh_X_test, oneh_y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set = {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Emotion Detection from Text (from [kaggle](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
