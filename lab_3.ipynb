{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laboratory work #3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all significant libraries for this project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow & Keras Libraries\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPool2D, RNN, GlobalMaxPool2D\n",
    "from keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomHeight, RandomWidth\n",
    "from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Import scikit-learn libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import libraries for text cleaning\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Who\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Possible Asteroid Impacts with Earth (from [kaggle](https://www.kaggle.com/datasets/nasa/asteroid-impacts))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Loading dataset to pandas DataFrame\n",
    "asteroid_df = pd.read_csv(\"datasets\\\\asteroid_dataset\\\\asteroid_classification.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n",
      "       'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for analysis from the dataframe\n",
    "asteroid_df.drop([\"Object Name\", \"Epoch (TDB)\", \"Perihelion Argument (deg)\", \"Node Longitude (deg)\",\n",
    "                  \"Mean Anomoly (deg)\", \"Perihelion Distance (AU)\", \"Aphelion Distance (AU)\",\n",
    "                  \"Minimum Orbit Intersection Distance (AU)\", \"Orbital Reference\"], axis=1, inplace=True)\n",
    "print(asteroid_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15634 entries, 0 to 15634\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Object Classification    15634 non-null  object \n",
      " 1   Orbit Axis (AU)          15634 non-null  float64\n",
      " 2   Orbit Eccentricity       15634 non-null  float64\n",
      " 3   Orbit Inclination (deg)  15634 non-null  float64\n",
      " 4   Orbital Period (yr)      15634 non-null  float64\n",
      " 5   Asteroid Magnitude       15634 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 855.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop null values of dataframe as we have only one null value\n",
    "asteroid_df.dropna(inplace=True)\n",
    "asteroid_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Change class names\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apollo Asteroid\" ,\n",
    "                                          \"Apollo\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Aten Asteroid\",\n",
    "                                          \"Aten\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Amor Asteroid\",\n",
    "                                          \"Amor\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apohele Asteroid\",\n",
    "                                          \"Apohele\", inplace=True)\n",
    "# Drop unnecessary class\n",
    "necessary_class = [\"Apollo\", \"Aten\", \"Amor\"]\n",
    "asteroid_df = asteroid_df[asteroid_df[\"Object Classification\"].isin(necessary_class)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, features):\n",
    "    \"\"\"Function to remove the outliers;\n",
    "    :param dataframe: pandas DataFrame with data;\n",
    "    :param features: list with features from dataframe.\n",
    "    \"\"\"\n",
    "    # Copy dataframe to another variable\n",
    "    dataframe_copy = dataframe.copy()\n",
    "\n",
    "    # Iterate through features\n",
    "    for feature in features:\n",
    "        if dataframe[feature].dtype == object:\n",
    "            continue\n",
    "        # Calculate q1, q3 and iqr\n",
    "        q3 = dataframe[feature].quantile(0.75)\n",
    "        q1 = dataframe[feature].quantile(0.25)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Get local minimum and maximum\n",
    "        local_min = q1 - (1.5 * iqr)\n",
    "        local_max = q3 + (1.5 * iqr)\n",
    "\n",
    "        # Remove the outliers\n",
    "        dataframe_copy = dataframe_copy[(dataframe_copy[feature] >= local_min) &\n",
    "                                        (dataframe_copy[feature] <= local_max)]\n",
    "\n",
    "    return dataframe_copy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the df\n",
    "asteroid_features = asteroid_df.columns.tolist()\n",
    "print(asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Remove outliers from the dataframe\n",
    "asteroid_df = remove_outliers(asteroid_df, asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apollo    6651\n",
      "Amor      5686\n",
      "Aten       965\n",
      "Name: Object Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View count of class names\n",
    "print(asteroid_df.iloc[:, 0].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "num_classes = len(necessary_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Object Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                  Amor         0.807646            0.590282   \n",
      "2                  Amor         0.758732            0.610967   \n",
      "4                  Amor         0.587438            0.469295   \n",
      "8                  Amor         0.570204            0.427279   \n",
      "9                Apollo         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Normalise dataset\n",
    "norm_asteroid_df = asteroid_df.copy()\n",
    "# apply normalization techniques\n",
    "for column in norm_asteroid_df:\n",
    "    if norm_asteroid_df[column].dtype == object:\n",
    "        continue\n",
    "    norm_asteroid_df[column] = norm_asteroid_df[column] / norm_asteroid_df[column].abs().max()\n",
    "# View normalised dataset\n",
    "print(norm_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Object_Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                      2         0.807646            0.590282   \n",
      "2                      2         0.758732            0.610967   \n",
      "4                      2         0.587438            0.469295   \n",
      "8                      2         0.570204            0.427279   \n",
      "9                      0         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Edit name of column \"Object Classification\" with _\n",
    "# For using this name in next cell\n",
    "number_asteroid_df = norm_asteroid_df.copy()\n",
    "number_asteroid_df.rename(columns={\"Object Classification\": \"Object_Classification\"}, inplace=True)\n",
    "# Replace string class to numbers\n",
    "obj_class = {\"Apollo\": 0, \"Aten\": 1, \"Amor\": 2}\n",
    "number_asteroid_df.Object_Classification = [obj_class[item] for item in number_asteroid_df.Object_Classification]\n",
    "# View new dataset\n",
    "print(number_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063   1.0     0.0   0.0  \n",
      "2             0.661017            0.425397   1.0     0.0   0.0  \n",
      "4             0.450847            0.561905   1.0     0.0   0.0  \n",
      "8             0.430508            0.419048   1.0     0.0   0.0  \n",
      "9             0.271186            0.451746   0.0     1.0   0.0  \n"
     ]
    }
   ],
   "source": [
    "# One-hot Encoding the Object Classification Feature\n",
    "one_hot = OneHotEncoder()\n",
    "# Copy our dataset\n",
    "onehot_asteroid_df = norm_asteroid_df.copy()\n",
    "# Fitting one-hot encoder\n",
    "encoded = one_hot.fit_transform(onehot_asteroid_df[[\"Object Classification\"]])\n",
    "onehot_asteroid_df[one_hot.categories_[0]] = encoded.toarray()\n",
    "# Drop unnecessary \"Object Classification\" feature\n",
    "onehot_asteroid_df.drop([\"Object Classification\"], axis=1, inplace=True)\n",
    "print(onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbit Axis (AU)            float64\n",
      "Orbit Eccentricity         float64\n",
      "Orbit Inclination (deg)    float64\n",
      "Orbital Period (yr)        float64\n",
      "Asteroid Magnitude         float64\n",
      "Amor                         int32\n",
      "Apollo                       int32\n",
      "Aten                         int32\n",
      "dtype: object\n",
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063     1       0     0  \n",
      "2             0.661017            0.425397     1       0     0  \n",
      "4             0.450847            0.561905     1       0     0  \n",
      "8             0.430508            0.419048     1       0     0  \n",
      "9             0.271186            0.451746     0       1     0  \n"
     ]
    }
   ],
   "source": [
    "# Change data type in one-hot encoded column\n",
    "column_dtype_dict = {\"Amor\": int,\n",
    "                     \"Apollo\": int,\n",
    "                     \"Aten\": int}\n",
    "norm_onehot_asteroid_df = onehot_asteroid_df.astype(column_dtype_dict)\n",
    "print(norm_onehot_asteroid_df.dtypes)\n",
    "print(norm_onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Dataset to Train & Test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Split Categorical Dataset\n",
    "x = norm_asteroid_df.drop([\"Object Classification\"], axis=1)\n",
    "y = norm_asteroid_df[\"Object Classification\"]\n",
    "# Split to train test sets\n",
    "catg_X_train, catg_X_test, catg_y_train, catg_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Split Numeric Dataset\n",
    "x = number_asteroid_df.drop([\"Object_Classification\"], axis=1)\n",
    "y = number_asteroid_df[\"Object_Classification\"]\n",
    "# Split to train test sets\n",
    "num_X_train, num_X_test, num_y_train, num_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Split One-Hot Dataset\n",
    "x = norm_onehot_asteroid_df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "y = norm_onehot_asteroid_df[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "# Split to train test sets\n",
    "# oneh_X_train, oneh_X_valid, oneh_X_test = np.split(x.sample(frac=1), [int(0.8*len(x)), int(0.9*len(x))])\n",
    "# oneh_y_train, oneh_y_valid, oneh_y_test = np.split(y.sample(frac=1), [int(0.8*len(y)), int(0.9*len(y))])\n",
    "oneh_train, oneh_valid, oneh_test = np.split(norm_onehot_asteroid_df.sample(frac=1), [int(0.8*len(norm_onehot_asteroid_df)), int(0.9*len(norm_onehot_asteroid_df))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    Function for converting dataframe var to tf dataset.\n",
    "    :param dataframe:\n",
    "    :param shuffle:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Copy dataframe to new vars\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "    labels = dataframe[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "    # Convert dataframe with data to tensor with concrete dtype -- float32\n",
    "    df = tf.constant(df, dtype=tf.float32)\n",
    "    # Make TensorFlow Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df, labels))\n",
    "    # Optimize TF Dataset for loading to model\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Define batch size variable\n",
    "batch_size = 32\n",
    "# Get TensorFlow Dataset object\n",
    "train_ds = df_to_dataset(oneh_train, batch_size=batch_size)\n",
    "valid_ds = df_to_dataset(oneh_valid, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(oneh_test, shuffle=False, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 5), dtype=float32, numpy=\n",
      "array([[0.691573  , 0.74822235, 0.05346096, 0.5762712 , 0.86031747],\n",
      "       [0.74713033, 0.60881275, 0.23767088, 0.64576274, 0.6952381 ],\n",
      "       [0.61165017, 0.52908856, 0.23881587, 0.4779661 , 0.7777778 ],\n",
      "       [0.3599437 , 0.35811248, 0.3107529 , 0.21694915, 0.6920635 ],\n",
      "       [0.51602435, 0.45593622, 0.32116282, 0.37118644, 0.74920636],\n",
      "       [0.49398512, 0.4771601 , 0.10288757, 0.34745762, 0.85714287],\n",
      "       [0.34473047, 0.37319544, 0.02777941, 0.20338982, 0.768254  ],\n",
      "       [0.35721937, 0.37621203, 0.14390735, 0.21355931, 0.73968256],\n",
      "       [0.33420062, 0.5287654 , 0.13169159, 0.19322033, 0.71746033],\n",
      "       [0.5683063 , 0.38149107, 0.15756781, 0.42881355, 0.6507937 ],\n",
      "       [0.41482753, 0.6149537 , 0.16654329, 0.2677966 , 0.7714286 ],\n",
      "       [0.4875264 , 0.65513897, 0.69243616, 0.34067798, 0.7015873 ],\n",
      "       [0.51841193, 0.6356389 , 0.69989526, 0.37288135, 0.584127  ],\n",
      "       [0.33769017, 0.17420815, 0.6442519 , 0.19661017, 0.73650795],\n",
      "       [0.6810126 , 0.5727214 , 0.5112309 , 0.56271183, 0.72063494],\n",
      "       [0.76436377, 0.56453353, 0.31297627, 0.6694915 , 0.5714286 ],\n",
      "       [0.9188833 , 0.6579401 , 0.67119366, 0.88135594, 0.5460318 ],\n",
      "       [0.72086686, 0.6156001 , 0.2252963 , 0.6118644 , 0.7936508 ],\n",
      "       [0.4911996 , 0.50107735, 0.27538633, 0.34406778, 0.5936508 ],\n",
      "       [0.57901984, 0.6752855 , 0.02421637, 0.44067797, 0.75555557],\n",
      "       [0.54188985, 0.326546  , 0.79395336, 0.4       , 0.64444447],\n",
      "       [0.6080688 , 0.8165266 , 0.04215709, 0.47457626, 0.71746033],\n",
      "       [0.6171294 , 0.856281  , 0.09354324, 0.48474577, 0.8       ],\n",
      "       [0.50583124, 0.5985779 , 0.7416143 , 0.35932204, 0.6698413 ],\n",
      "       [0.52667665, 0.38181427, 0.90027076, 0.38305086, 0.584127  ],\n",
      "       [0.478374  , 0.2340013 , 0.54686385, 0.33050847, 0.64761907],\n",
      "       [0.4604671 , 0.40088344, 0.440695  , 0.31355932, 0.6603175 ],\n",
      "       [0.86047935, 0.679164  , 0.04443426, 0.7983051 , 0.7746032 ],\n",
      "       [0.5658575 , 0.4154277 , 0.5599531 , 0.42542374, 0.6603175 ],\n",
      "       [0.41403165, 0.13725491, 0.5191203 , 0.2661017 , 0.63809526],\n",
      "       [0.39539012, 0.5279035 , 0.1425472 , 0.24915254, 0.83492064],\n",
      "       [0.7672411 , 0.63520795, 0.30668524, 0.67288136, 0.784127  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(32, 3), dtype=int32, numpy=\n",
      "array([[1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 0, 1]])>)\n"
     ]
    }
   ],
   "source": [
    "# View new train dataset\n",
    "for onehot in train_ds.take(1):\n",
    "    print(onehot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def model_compile(model, loss_func, learn_rate=0.001):\n",
    "    \"\"\"\n",
    "    Help function for compiling model;\n",
    "    :param model: built model;\n",
    "    :param loss_func: loss function for compiling;\n",
    "    :param learn_rate: learning rate for optimizer.\n",
    "    \"\"\"\n",
    "    model.compile(loss=loss_func,\n",
    "                  optimizer=Adam(learning_rate=learn_rate),\n",
    "                  metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Set Input layer for input data\n",
    "input_shape = len(asteroid_features)-1\n",
    "inputs = Input(shape=(input_shape,), name=\"model_inputs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Set Dense (Fully Connected) layer with 100 hidden unit and \"relu\" activation function\n",
    "x = Dense(100, activation=\"relu\")(inputs)\n",
    "# Set another Dense layer with 10 hidden unit and \"relu\" activation function\n",
    "x = Dense(10, activation=\"relu\")(x)\n",
    "# Set output layer with num_classes hidden unit and \"sigmoid\" activation function\n",
    "outputs = Dense(num_classes, activation=\"sigmoid\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Define our model\n",
    "onehot_model = Model(inputs, outputs, name=\"onehot_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"onehot_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_inputs (InputLayer)   [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               600       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,643\n",
      "Trainable params: 1,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View summary of the model\n",
    "onehot_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(onehot_model, CategoricalCrossentropy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "333/333 [==============================] - 3s 4ms/step - loss: 0.7306 - accuracy: 0.6950 - val_loss: 0.5407 - val_accuracy: 0.7917\n",
      "Epoch 2/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.4504 - accuracy: 0.8159 - val_loss: 0.3928 - val_accuracy: 0.8098\n",
      "Epoch 3/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.3297 - accuracy: 0.8551 - val_loss: 0.2794 - val_accuracy: 0.9038\n",
      "Epoch 4/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.2540 - accuracy: 0.9100 - val_loss: 0.2196 - val_accuracy: 0.9308\n",
      "Epoch 5/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.2061 - accuracy: 0.9354 - val_loss: 0.1861 - val_accuracy: 0.9338\n",
      "Epoch 6/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1825 - accuracy: 0.9387 - val_loss: 0.1567 - val_accuracy: 0.9549\n",
      "Epoch 7/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1591 - accuracy: 0.9492 - val_loss: 0.1404 - val_accuracy: 0.9624\n",
      "Epoch 8/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.9509 - val_loss: 0.1396 - val_accuracy: 0.9504\n",
      "Epoch 9/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1376 - accuracy: 0.9524 - val_loss: 0.1197 - val_accuracy: 0.9639\n",
      "Epoch 10/10\n",
      "333/333 [==============================] - 1s 4ms/step - loss: 0.1278 - accuracy: 0.9579 - val_loss: 0.1168 - val_accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "onehot_history = onehot_model.fit(train_ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10,\n",
    "                                  validation_data=valid_ds,\n",
    "                                  use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9609\n",
      "Model loss on the test set: 0.12188560515642166\n",
      "Model accuracy on the test set = 0.960931658744812\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = onehot_model.evaluate(test_ds)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set = {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last best result on this data is that:\n",
    "**model** - GridSearch(SVM)\n",
    "**result** - accuracy = 0.99\n",
    "\n",
    "Now result is\n",
    "**model** - Fully connected feed-forward network\n",
    "**result** - accuracy = 0.96\n",
    "\n",
    "In summary, we can say model (GridSearch(SVM)) have best fit on this data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Food Classification Dataset (from [kaggle]())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def prepare_ds(dataset, shuffle=False, batch=32):\n",
    "    \"\"\"\n",
    "    Help function for prepare dataset for better performance;\n",
    "    :param dataset: TensorFlow Dataset object;\n",
    "    :param shuffle: bool for define make shuffle or not;\n",
    "    :param batch: size of batch in dataset;\n",
    "    :return: normalized and prepared dataset.\n",
    "    \"\"\"\n",
    "    # Shuffle our dataset\n",
    "    if shuffle:\n",
    "        dataset.shuffle(buffer_size=len(dataset))\n",
    "    # Batching our dataset\n",
    "    dataset.batch(batch)\n",
    "    # Augmenting our dataset\n",
    "    norm_layer = Rescaling(1./255)\n",
    "    ds = dataset.map(lambda x, y: (norm_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    image_batch, labels_batch = next(iter(ds))\n",
    "    first_image = image_batch[0]\n",
    "    print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# Define train and test dir\n",
    "train_dir = pathlib.Path(\"datasets\\\\10_food_classes_all_data\\\\train\\\\\")\n",
    "test_dir = pathlib.Path(\"datasets\\\\10_food_classes_all_data\\\\test\\\\\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('datasets/10_food_classes_all_data/train')"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 files belonging to 10 classes.\n",
      "Found 2500 files belonging to 10 classes.\n",
      "Using 1250 files for validation.\n",
      "Found 2500 files belonging to 10 classes.\n",
      "Using 1250 files for training.\n"
     ]
    }
   ],
   "source": [
    "# Define some important variable\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "random_seed = 17\n",
    "# Set train and test datasets\n",
    "train_ds = image_dataset_from_directory(train_dir,\n",
    "                                        label_mode=\"categorical\",\n",
    "                                        seed=random_seed,\n",
    "                                        image_size=image_size,\n",
    "                                        batch_size=batch_size)\n",
    "val_ds = image_dataset_from_directory(test_dir,\n",
    "                                      label_mode=\"categorical\",\n",
    "                                      validation_split=0.5,\n",
    "                                      subset=\"validation\",\n",
    "                                      seed=random_seed,\n",
    "                                      image_size=image_size,\n",
    "                                      batch_size=batch_size)\n",
    "test_ds = image_dataset_from_directory(test_dir,\n",
    "                                       label_mode=\"categorical\",\n",
    "                                       validation_split=0.5,\n",
    "                                       subset=\"training\",\n",
    "                                       seed=random_seed,\n",
    "                                       image_size=image_size,\n",
    "                                       batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View on train Dataset\n",
    "train_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(32, 224, 224, 3), dtype=float32, numpy=\n array([[[[ 77.45918  , 101.77041  ,  62.       ],\n          [ 77.994896 , 102.85715  ,  62.903065 ],\n          [ 76.       , 102.15306  ,  63.07653  ],\n          ...,\n          [178.78587  , 204.2144   , 185.2144   ],\n          [181.7143   , 204.7143   , 184.7143   ],\n          [182.64282  , 205.64282  , 185.64282  ]],\n \n         [[118.45409  , 136.76021  ,  94.78572  ],\n          [115.9898   , 136.92348  ,  94.06633  ],\n          [110.44388  , 132.12755  ,  91.       ],\n          ...,\n          [182.50015  , 207.92868  , 188.92868  ],\n          [184.92857  , 207.92857  , 187.92857  ],\n          [185.49997  , 206.49997  , 187.49997  ]],\n \n         [[143.91837  , 158.35715  , 113.42347  ],\n          [145.05612  , 160.2296   , 115.071434 ],\n          [140.16327  , 156.47449  , 111.52041  ],\n          ...,\n          [175.59712  , 201.02565  , 182.02565  ],\n          [182.77045  , 203.77045  , 184.77045  ],\n          [188.49489  , 207.49489  , 188.49489  ]],\n \n         ...,\n \n         [[140.86215  , 149.43362  ,  96.86215  ],\n          [148.44382  , 157.94386  , 105.37238  ],\n          [155.02547  , 164.42857  , 115.23978  ],\n          ...,\n          [ 26.949179 ,  62.94918  ,  34.94918  ],\n          [ 42.74006  ,  78.74006  ,  50.74006  ],\n          [ 59.286285 ,  96.71481  ,  66.14334  ]],\n \n         [[134.86223  , 145.95409  ,  89.95409  ],\n          [141.70917  , 154.64285  , 100.642845 ],\n          [149.64282  , 163.85715  , 111.21427  ],\n          ...,\n          [ 47.05644  ,  82.05644  ,  52.05644  ],\n          [ 63.214508 ,  98.21451  ,  68.21451  ],\n          [ 81.18902  , 116.18902  ,  86.18902  ]],\n \n         [[133.28572  , 146.28572  ,  90.28571  ],\n          [138.47446  , 153.38263  ,  98.428535 ],\n          [144.06625  , 161.14279  , 107.57136  ],\n          ...,\n          [ 69.5719   , 104.5719   ,  74.5719   ],\n          [ 82.61764  , 117.61764  ,  87.61764  ],\n          [ 94.01549  , 129.01549  ,  99.01549  ]]],\n \n \n        [[[  9.       ,  12.       ,  27.       ],\n          [  9.       ,  12.       ,  27.       ],\n          [  9.       ,  12.       ,  27.       ],\n          ...,\n          [  9.642858 ,  11.642858 ,  26.642857 ],\n          [  9.642858 ,  11.642858 ,  26.642857 ],\n          [  9.642858 ,  11.642858 ,  26.642857 ]],\n \n         [[  9.       ,  12.       ,  27.       ],\n          [  9.       ,  12.       ,  27.       ],\n          [  9.       ,  12.       ,  27.       ],\n          ...,\n          [  8.071428 ,  10.071428 ,  23.071428 ],\n          [  8.071428 ,  10.071428 ,  23.071428 ],\n          [  8.071428 ,  10.071428 ,  23.071428 ]],\n \n         [[  9.214286 ,  12.       ,  26.571428 ],\n          [  9.214286 ,  12.       ,  26.571428 ],\n          [  9.214286 ,  12.       ,  26.571428 ],\n          ...,\n          [ 10.785714 ,  12.785714 ,  25.571428 ],\n          [ 10.785714 ,  12.785714 ,  25.571428 ],\n          [ 10.785714 ,  12.785714 ,  25.571428 ]],\n \n         ...,\n \n         [[ 37.95415  ,  41.95415  ,  42.95415  ],\n          [ 30.071648 ,  34.071648 ,  35.071648 ],\n          [ 36.571472 ,  40.571472 ,  41.571472 ],\n          ...,\n          [ 35.04591  ,  39.04591  ,  40.04591  ],\n          [ 32.428772 ,  36.428772 ,  37.428772 ],\n          [ 38.872627 ,  42.872627 ,  43.872627 ]],\n \n         [[ 81.7983   ,  85.75237  ,  88.58908  ],\n          [ 87.578835 ,  91.57373  ,  94.359406 ],\n          [ 81.90544  ,  85.90544  ,  88.72173  ],\n          ...,\n          [ 81.54833  ,  85.54833  ,  88.390144 ],\n          [ 87.578835 ,  91.50739  ,  94.29307  ],\n          [ 80.65022  ,  84.65022  ,  87.46141  ]],\n \n         [[244.77046  , 248.77046  , 251.77046  ],\n          [240.44911  , 244.44911  , 247.44911  ],\n          [244.48488  , 248.48488  , 251.20424  ],\n          ...,\n          [242.9083   , 246.9083   , 249.9083   ],\n          [244.16347  , 248.16347  , 251.16347  ],\n          [243.94395  , 247.94395  , 250.94395  ]]],\n \n \n        [[[138.38776  , 125.10204  , 139.7449   ],\n          [133.88776  , 110.102036 , 125.36734  ],\n          [143.38264  , 106.03062  , 123.01531  ],\n          ...,\n          [158.72446  , 146.58147  , 157.51518  ],\n          [177.65785  , 141.87201  , 153.68323  ],\n          [130.17838  ,  73.67824  ,  85.76496  ]],\n \n         [[138.4847   , 125.198975 , 139.84184  ],\n          [145.75     , 119.37755  , 136.16837  ],\n          [141.16837  , 105.92857  , 123.14285  ],\n          ...,\n          [205.25522  , 205.954    , 220.84186  ],\n          [167.71376  , 140.58095  , 165.64223  ],\n          [132.42322  ,  83.78022  , 115.637405 ]],\n \n         [[138.64796  , 120.79591  , 134.65306  ],\n          [136.21939  , 109.09184  , 126.21938  ],\n          [140.63776  , 105.994896 , 123.20918  ],\n          ...,\n          [213.83199  , 206.32669  , 211.23499  ],\n          [143.95898  , 108.10166  , 127.67318  ],\n          [124.39259  ,  65.67813  ,  96.831276 ]],\n \n         ...,\n \n         [[121.05612  ,  87.627594 ,  94.627594 ],\n          [128.01529  ,  94.586754 , 103.158226 ],\n          [125.459076 ,  92.122375 , 100.69385  ],\n          ...,\n          [210.64279  , 251.21426  , 247.       ],\n          [211.71423  , 251.49997  , 250.07144  ],\n          [212.36214  , 252.14787  , 250.71935  ]],\n \n         [[127.239815 ,  94.239815 , 103.239815 ],\n          [132.2295   ,  99.22952  , 108.22952  ],\n          [120.63274  ,  89.41846  ,  98.06131  ],\n          ...,\n          [204.9132   , 251.19896  , 249.12752  ],\n          [205.85202  , 252.13779  , 250.06635  ],\n          [206.35706  , 250.71426  , 249.57138  ]],\n \n         [[129.63264  ,  96.63264  , 105.63264  ],\n          [119.31623  ,  88.31623  ,  96.31623  ],\n          [130.08662  ,  98.08662  , 109.08662  ],\n          ...,\n          [196.85194  , 249.92348  , 247.5663   ],\n          [199.92847  , 253.       , 250.64282  ],\n          [198.57129  , 251.64282  , 249.28564  ]]],\n \n \n        ...,\n \n \n        [[[196.22449  , 236.22449  , 235.22449  ],\n          [191.5      , 233.5      , 232.5      ],\n          [192.40306  , 232.69897  , 235.4796   ],\n          ...,\n          [254.       , 254.       , 254.       ],\n          [254.       , 254.       , 254.       ],\n          [254.       , 254.       , 254.       ]],\n \n         [[183.7398   , 223.7398   , 223.7398   ],\n          [164.24489  , 208.24489  , 207.31122  ],\n          [165.69388  , 209.93367  , 212.87755  ],\n          ...,\n          [254.       , 254.       , 254.       ],\n          [254.       , 254.       , 254.       ],\n          [254.       , 254.       , 254.       ]],\n \n         [[155.00511  , 194.44388  , 196.29591  ],\n          [111.35714  , 158.67346  , 158.7143   ],\n          [ 95.658165 , 145.70918  , 146.96939  ],\n          ...,\n          [253.21428  , 254.78572  , 254.       ],\n          [253.21428  , 254.78572  , 254.       ],\n          [253.21428  , 254.78572  , 254.       ]],\n \n         ...,\n \n         [[ 94.923485 ,  89.27551  ,  67.35204  ],\n          [ 86.78573  ,  86.642876 ,  62.714302 ],\n          [ 87.688805 ,  89.26023  ,  65.26023  ],\n          ...,\n          [200.5715   , 238.14291  , 249.35721  ],\n          [207.75517  , 244.1837   , 250.05614  ],\n          [214.85211  , 249.63782  , 251.56128  ]],\n \n         [[ 92.16829  ,  85.45401  ,  63.81115  ],\n          [ 89.153076 ,  89.010216 ,  65.08164  ],\n          [ 90.015305 ,  93.04594  ,  68.31633  ],\n          ...,\n          [201.88788  , 236.42862  , 248.42862  ],\n          [209.28067  , 242.28061  , 248.93874  ],\n          [215.68884  , 246.90308  , 248.87758  ]],\n \n         [[ 93.7755   ,  84.70407  ,  62.06121  ],\n          [ 83.4745   ,  83.18878  ,  57.260216 ],\n          [ 87.77041  ,  90.91326  ,  64.12755  ],\n          ...,\n          [204.70927  , 237.1378   , 249.92354  ],\n          [213.05103  , 242.76022  , 248.19385  ],\n          [218.4133   , 247.1276   , 249.05609  ]]],\n \n \n        [[[ 18.32653  ,  14.32653  ,  15.32653  ],\n          [ 15.265306 ,  11.265306 ,  12.265306 ],\n          [ 17.362246 ,  13.362246 ,  12.362246 ],\n          ...,\n          [ 17.581638 ,  14.581639 ,   8.010167 ],\n          [ 17.642857 ,  14.642858 ,   7.642857 ],\n          [ 16.770386 ,  13.770386 ,   4.7703857]],\n \n         [[ 14.02551  ,  10.02551  ,  11.02551  ],\n          [ 16.857143 ,  12.857143 ,  13.857143 ],\n          [ 20.19898  ,  16.19898  ,  15.198979 ],\n          ...,\n          [ 18.770432 ,  15.7704315,   9.198959 ],\n          [ 18.994884 ,  15.994884 ,   8.994884 ],\n          [ 16.571394 ,  13.571394 ,   6.571394 ]],\n \n         [[ 17.204082 ,  13.204082 ,  14.204082 ],\n          [ 14.357142 ,  10.357142 ,  11.357142 ],\n          [ 15.765306 ,  11.       ,  10.382653 ],\n          ...,\n          [ 20.122458 ,  17.122458 ,  12.122458 ],\n          [ 16.198977 ,  13.1989765,   6.6275487],\n          [ 18.714355 ,  15.714355 ,   9.142927 ]],\n \n         ...,\n \n         [[223.2092   , 208.2092   , 169.2092   ],\n          [223.14285  , 208.14285  , 169.14285  ],\n          [223.38263  , 208.38263  , 169.38263  ],\n          ...,\n          [218.12245  , 196.12245  , 173.12245  ],\n          [218.       , 196.       , 173.       ],\n          [217.92348  , 195.92348  , 172.92348  ]],\n \n         [[221.21428  , 206.21428  , 167.21428  ],\n          [220.13774  , 205.13774  , 166.13774  ],\n          [222.0714   , 207.0714   , 168.0714   ],\n          ...,\n          [213.92856  , 191.92856  , 167.92856  ],\n          [214.06635  , 192.06635  , 168.06635  ],\n          [217.18878  , 195.18878  , 171.18878  ]],\n \n         [[222.00003  , 206.00003  , 170.00003  ],\n          [217.31122  , 201.31122  , 165.31122  ],\n          [216.42339  , 200.42339  , 164.42339  ],\n          ...,\n          [218.57661  , 194.57661  , 170.57661  ],\n          [217.28574  , 193.28574  , 169.28574  ],\n          [215.28564  , 191.28564  , 167.28564  ]]],\n \n \n        [[[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]],\n \n         [[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]],\n \n         [[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]],\n \n         ...,\n \n         [[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]],\n \n         [[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]],\n \n         [[  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          ...,\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ],\n          [  0.       ,   0.       ,   2.       ]]]], dtype=float32)>,\n <tf.Tensor: shape=(32, 10), dtype=float32, numpy=\n array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)>)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View on test Dataset\n",
    "next(iter(train_ds))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "['chicken_curry',\n 'chicken_wings',\n 'fried_rice',\n 'grilled_salmon',\n 'hamburger',\n 'ice_cream',\n 'pizza',\n 'ramen',\n 'steak',\n 'sushi']"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# View number of classes\n",
    "num_image_classes = len(train_ds.class_names)\n",
    "print(num_image_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9974792\n",
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for better performance\n",
    "train_ds = prepare_ds(train_ds, shuffle=True)\n",
    "val_ds = prepare_ds(val_ds)\n",
    "test_ds = prepare_ds(test_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "# Build data augmentation layer with horizontal flipping, rotations, zooms\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1),\n",
    "    # RandomHeight(0.1),\n",
    "    # RandomWidth(0.1)\n",
    "], name=\"data_augmentation\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape = (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set Input layer for input data\n",
    "image_shape = None\n",
    "for images in train_ds.take(1):\n",
    "    image_shape = images[0].shape[1:]\n",
    "print(f\"image shape = {image_shape}\")\n",
    "inputs = Input(shape=image_shape, name=\"model_inputs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Set data augmentation layer\n",
    "x = data_augmentation(inputs)\n",
    "# Set Convolution layers\n",
    "x = Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "# Set Max Pooling 2D layer\n",
    "x = MaxPool2D()(x)\n",
    "# Set another Convolution layers\n",
    "x = Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "# Set another Max Pooling 2D layer\n",
    "x = MaxPool2D()(x)\n",
    "# Set Flatten and finish Dense (Fully connected) layers\n",
    "x = GlobalMaxPool2D(name=\"global_average_pooling_layer\")(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "outputs = Dense(num_image_classes, activation=\"softmax\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "# Define model object\n",
    "conv_model = Model(inputs, outputs, name=\"convolution_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(conv_model, CategoricalCrossentropy(), learn_rate=0.005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "235/235 [==============================] - 70s 277ms/step - loss: 2.2884 - accuracy: 0.1276 - val_loss: 2.2593 - val_accuracy: 0.1488\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 63s 267ms/step - loss: 2.2412 - accuracy: 0.1635 - val_loss: 2.2343 - val_accuracy: 0.1704\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 63s 268ms/step - loss: 2.2151 - accuracy: 0.1837 - val_loss: 2.2029 - val_accuracy: 0.1744\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 64s 269ms/step - loss: 2.1973 - accuracy: 0.1949 - val_loss: 2.1869 - val_accuracy: 0.1904\n",
      "Epoch 5/10\n",
      " 59/235 [======>.......................] - ETA: 46s - loss: 2.1857 - accuracy: 0.2034"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [141], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Fit model with train and test data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m conv_history \u001B[38;5;241m=\u001B[39m \u001B[43mconv_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                              \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m      7\u001B[0m \u001B[43m                              \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Fit model with train and test data\n",
    "conv_history = conv_model.fit(train_ds,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_ds,\n",
    "                              use_multiprocessing=True\n",
    "                              )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 7s 89ms/step - loss: 5.7694 - accuracy: 0.1672\n",
      "Model loss on the test set: 5.769413471221924\n",
      "Model accuracy on the test set: 0.1671999990940094\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = conv_model.evaluate(test_ds)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In summary, we have that result:\n",
    "**model** -- Convolution Neural Network (CNN)\n",
    "**result** -- accuracy = ; loss = ;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Emotion Detection from Text (from [kaggle](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
