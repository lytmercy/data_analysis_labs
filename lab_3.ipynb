{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laboratory work #3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all significant libraries for this project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow & Keras Libraries\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers import Conv2D, RNN\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "# Import scikit-learn libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import libraries for text cleaning\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Who\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Possible Asteroid Impacts with Earth (from [kaggle](https://www.kaggle.com/datasets/nasa/asteroid-impacts))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Loading dataset to pandas DataFrame\n",
    "asteroid_df = pd.read_csv(\"datasets\\\\asteroid_dataset\\\\asteroid_classification.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n",
      "       'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns for analysis from the dataframe\n",
    "asteroid_df.drop([\"Object Name\", \"Epoch (TDB)\", \"Perihelion Argument (deg)\", \"Node Longitude (deg)\",\n",
    "                  \"Mean Anomoly (deg)\", \"Perihelion Distance (AU)\", \"Aphelion Distance (AU)\",\n",
    "                  \"Minimum Orbit Intersection Distance (AU)\", \"Orbital Reference\"], axis=1, inplace=True)\n",
    "print(asteroid_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15634 entries, 0 to 15634\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Object Classification    15634 non-null  object \n",
      " 1   Orbit Axis (AU)          15634 non-null  float64\n",
      " 2   Orbit Eccentricity       15634 non-null  float64\n",
      " 3   Orbit Inclination (deg)  15634 non-null  float64\n",
      " 4   Orbital Period (yr)      15634 non-null  float64\n",
      " 5   Asteroid Magnitude       15634 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 855.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop null values of dataframe as we have only one null value\n",
    "asteroid_df.dropna(inplace=True)\n",
    "asteroid_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Change class names\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apollo Asteroid\" ,\n",
    "                                          \"Apollo\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Aten Asteroid\",\n",
    "                                          \"Aten\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Amor Asteroid\",\n",
    "                                          \"Amor\", inplace=True)\n",
    "asteroid_df[\"Object Classification\"].mask(asteroid_df[\"Object Classification\"] == \"Apohele Asteroid\",\n",
    "                                          \"Apohele\", inplace=True)\n",
    "# Drop unnecessary class\n",
    "necessary_class = [\"Apollo\", \"Aten\", \"Amor\"]\n",
    "asteroid_df = asteroid_df[asteroid_df[\"Object Classification\"].isin(necessary_class)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe, features):\n",
    "    \"\"\"Function to remove the outliers;\n",
    "    :param dataframe: pandas DataFrame with data;\n",
    "    :param features: list with features from dataframe.\n",
    "    \"\"\"\n",
    "    # Copy dataframe to another variable\n",
    "    dataframe_copy = dataframe.copy()\n",
    "\n",
    "    # Iterate through features\n",
    "    for feature in features:\n",
    "        if dataframe[feature].dtype == object:\n",
    "            continue\n",
    "        # Calculate q1, q3 and iqr\n",
    "        q3 = dataframe[feature].quantile(0.75)\n",
    "        q1 = dataframe[feature].quantile(0.25)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        # Get local minimum and maximum\n",
    "        local_min = q1 - (1.5 * iqr)\n",
    "        local_max = q3 + (1.5 * iqr)\n",
    "\n",
    "        # Remove the outliers\n",
    "        dataframe_copy = dataframe_copy[(dataframe_copy[feature] >= local_min) &\n",
    "                                        (dataframe_copy[feature] <= local_max)]\n",
    "\n",
    "    return dataframe_copy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object Classification', 'Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the df\n",
    "asteroid_features = asteroid_df.columns.tolist()\n",
    "print(asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Remove outliers from the dataframe\n",
    "asteroid_df = remove_outliers(asteroid_df, asteroid_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apollo    6651\n",
      "Amor      5686\n",
      "Aten       965\n",
      "Name: Object Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View count of class names\n",
    "print(asteroid_df.iloc[:, 0].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "num_classes = len(necessary_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Object Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                  Amor         0.807646            0.590282   \n",
      "2                  Amor         0.758732            0.610967   \n",
      "4                  Amor         0.587438            0.469295   \n",
      "8                  Amor         0.570204            0.427279   \n",
      "9                Apollo         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Normalise dataset\n",
    "norm_asteroid_df = asteroid_df.copy()\n",
    "# apply normalization techniques\n",
    "for column in norm_asteroid_df:\n",
    "    if norm_asteroid_df[column].dtype == object:\n",
    "        continue\n",
    "    norm_asteroid_df[column] = norm_asteroid_df[column] / norm_asteroid_df[column].abs().max()\n",
    "# View normalised dataset\n",
    "print(norm_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Object_Classification  Orbit Axis (AU)  Orbit Eccentricity  \\\n",
      "1                      2         0.807646            0.590282   \n",
      "2                      2         0.758732            0.610967   \n",
      "4                      2         0.587438            0.469295   \n",
      "8                      2         0.570204            0.427279   \n",
      "9                      0         0.418531            0.469619   \n",
      "\n",
      "   Orbit Inclination (deg)  Orbital Period (yr)  Asteroid Magnitude  \n",
      "1                 0.296677             0.727119            0.492063  \n",
      "2                 0.239656             0.661017            0.425397  \n",
      "4                 0.304293             0.450847            0.561905  \n",
      "8                 0.216477             0.430508            0.419048  \n",
      "9                 0.240286             0.271186            0.451746  \n"
     ]
    }
   ],
   "source": [
    "# Edit name of column \"Object Classification\" with _\n",
    "# For using this name in next cell\n",
    "number_asteroid_df = norm_asteroid_df.copy()\n",
    "number_asteroid_df.rename(columns={\"Object Classification\": \"Object_Classification\"}, inplace=True)\n",
    "# Replace string class to numbers\n",
    "obj_class = {\"Apollo\": 0, \"Aten\": 1, \"Amor\": 2}\n",
    "number_asteroid_df.Object_Classification = [obj_class[item] for item in number_asteroid_df.Object_Classification]\n",
    "# View new dataset\n",
    "print(number_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063   1.0     0.0   0.0  \n",
      "2             0.661017            0.425397   1.0     0.0   0.0  \n",
      "4             0.450847            0.561905   1.0     0.0   0.0  \n",
      "8             0.430508            0.419048   1.0     0.0   0.0  \n",
      "9             0.271186            0.451746   0.0     1.0   0.0  \n"
     ]
    }
   ],
   "source": [
    "# One-hot Encoding the Object Classification Feature\n",
    "one_hot = OneHotEncoder()\n",
    "# Copy our dataset\n",
    "onehot_asteroid_df = norm_asteroid_df.copy()\n",
    "# Fitting one-hot encoder\n",
    "encoded = one_hot.fit_transform(onehot_asteroid_df[[\"Object Classification\"]])\n",
    "onehot_asteroid_df[one_hot.categories_[0]] = encoded.toarray()\n",
    "# Drop unnecessary \"Object Classification\" feature\n",
    "onehot_asteroid_df.drop([\"Object Classification\"], axis=1, inplace=True)\n",
    "print(onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbit Axis (AU)            float64\n",
      "Orbit Eccentricity         float64\n",
      "Orbit Inclination (deg)    float64\n",
      "Orbital Period (yr)        float64\n",
      "Asteroid Magnitude         float64\n",
      "Amor                         int32\n",
      "Apollo                       int32\n",
      "Aten                         int32\n",
      "dtype: object\n",
      "   Orbit Axis (AU)  Orbit Eccentricity  Orbit Inclination (deg)  \\\n",
      "1         0.807646            0.590282                 0.296677   \n",
      "2         0.758732            0.610967                 0.239656   \n",
      "4         0.587438            0.469295                 0.304293   \n",
      "8         0.570204            0.427279                 0.216477   \n",
      "9         0.418531            0.469619                 0.240286   \n",
      "\n",
      "   Orbital Period (yr)  Asteroid Magnitude  Amor  Apollo  Aten  \n",
      "1             0.727119            0.492063     1       0     0  \n",
      "2             0.661017            0.425397     1       0     0  \n",
      "4             0.450847            0.561905     1       0     0  \n",
      "8             0.430508            0.419048     1       0     0  \n",
      "9             0.271186            0.451746     0       1     0  \n"
     ]
    }
   ],
   "source": [
    "# Change data type in one-hot encoded column\n",
    "column_dtype_dict = {\"Amor\": int,\n",
    "                     \"Apollo\": int,\n",
    "                     \"Aten\": int}\n",
    "norm_onehot_asteroid_df = onehot_asteroid_df.astype(column_dtype_dict)\n",
    "print(norm_onehot_asteroid_df.dtypes)\n",
    "print(norm_onehot_asteroid_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Dataset to Train & Test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Split Categorical Dataset\n",
    "x = norm_asteroid_df.drop([\"Object Classification\"], axis=1)\n",
    "y = norm_asteroid_df[\"Object Classification\"]\n",
    "# Split to train test sets\n",
    "catg_X_train, catg_X_test, catg_y_train, catg_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Split Numeric Dataset\n",
    "x = number_asteroid_df.drop([\"Object_Classification\"], axis=1)\n",
    "y = number_asteroid_df[\"Object_Classification\"]\n",
    "# Split to train test sets\n",
    "num_X_train, num_X_test, num_y_train, num_y_test = train_test_split(x, y, test_size=0.20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Split One-Hot Dataset\n",
    "x = norm_onehot_asteroid_df.drop([\"Apollo\", \"Aten\", \"Amor\"], axis=1)\n",
    "y = norm_onehot_asteroid_df[[\"Apollo\", \"Aten\", \"Amor\"]]\n",
    "# Split to train test sets\n",
    "oneh_X_train, oneh_X_valid, oneh_X_test = np.split(x.sample(frac=1), [int(0.8*len(x)), int(0.9*len(x))])\n",
    "oneh_y_train, oneh_y_valid, oneh_y_test = np.split(y.sample(frac=1), [int(0.8*len(y)), int(0.9*len(y))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def xy_df_to_dataset(x, y, shuffle=True, batch_size=32):\n",
    "    \"\"\"\n",
    "    Function for converting dataframe var to tf dataset.\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param shuffle:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = x.copy()\n",
    "    labels = y.copy()\n",
    "    df = {key: value[:, tf.newaxis] for key, value in df.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Who\\AppData\\Local\\Temp\\ipykernel_8736\\2331624636.py:12: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:, tf.newaxis] for key, value in df.items()}\n",
      "C:\\Users\\Who\\AppData\\Local\\Temp\\ipykernel_8736\\2331624636.py:12: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:, tf.newaxis] for key, value in df.items()}\n",
      "C:\\Users\\Who\\AppData\\Local\\Temp\\ipykernel_8736\\2331624636.py:12: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:, tf.newaxis] for key, value in df.items()}\n"
     ]
    }
   ],
   "source": [
    "# Define batch size variable\n",
    "batch_size = 32\n",
    "# Get TensorFlow Dataset object\n",
    "train_ds = xy_df_to_dataset(oneh_X_train, oneh_y_train, batch_size=batch_size)\n",
    "valid_ds = xy_df_to_dataset(oneh_X_valid, oneh_y_valid, shuffle=False, batch_size=batch_size)\n",
    "test_ds = xy_df_to_dataset(oneh_X_test, oneh_y_test, shuffle=False, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def model_compile(model, learn_rate=0.001):\n",
    "    \"\"\"\n",
    "    Help function for compiling model;\n",
    "    :param model: built model;\n",
    "    :param learn_rate: learning rate for optimizer;\n",
    "    \"\"\"\n",
    "    model.compile(loss=CategoricalCrossentropy,\n",
    "                  optimizer=Adam(learning_rate=learn_rate),\n",
    "                  metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Set Input layer for input data\n",
    "input_shape = len(asteroid_features)-1\n",
    "inputs = Input(shape=(input_shape,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Set Dense (Fully Connected) layer with 100 hidden unit and \"relu\" activation function\n",
    "x = Dense(100, activation=\"relu\")(inputs)\n",
    "# Set another Dense layer with 10 hidden unit and \"relu\" activation function\n",
    "x = Dense(10, activation=\"relu\")(x)\n",
    "# Set output layer with num_classes hidden unit and \"sigmoid\" activation function\n",
    "outputs = Dense(num_classes, activation=\"sigmoid\")(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Define our model\n",
    "onehot_model = Model(inputs, outputs, name=\"onehot_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"onehot_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               600       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,643\n",
      "Trainable params: 1,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View summary of the model\n",
    "onehot_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_compile(onehot_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_4\". You passed a data dictionary with keys ['Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']. Expected the following keys: ['input_4']\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [50], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Fitting the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m onehot_history \u001B[38;5;241m=\u001B[39m \u001B[43monehot_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9mjfgkyy.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_4\". You passed a data dictionary with keys ['Orbit Axis (AU)', 'Orbit Eccentricity', 'Orbit Inclination (deg)', 'Orbital Period (yr)', 'Asteroid Magnitude']. Expected the following keys: ['input_4']\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "onehot_history = onehot_model.fit(train_ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=10,\n",
    "                                  validation_data=valid_ds,\n",
    "                                  use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\losses.py\", line 158, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.CategoricalCrossentropy object at 0x0000026144355600> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [37], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m \u001B[43monehot_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43moneh_X_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moneh_y_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loss on the test set: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel accuracy on the test set = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileookg6aid.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1667, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\losses.py\", line 158, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"D:\\.main\\.code\\data_analysis_labs\\venv\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.CategoricalCrossentropy object at 0x0000026144355600> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = onehot_model.evaluate(oneh_X_test, oneh_y_test)\n",
    "print(f\"Model loss on the test set: {loss}\")\n",
    "print(f\"Model accuracy on the test set = {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Downloading Dataset\n",
    "Our dataset is Emotion Detection from Text (from [kaggle](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
